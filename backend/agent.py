from datetime import datetime
import json
import os
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
# from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

documents_path =  os.path.join(os.path.dirname(__file__), "documents")

model_name = "BAAI/bge-m3" # "jhgan/ko-sroberta-multitask" #
# - NVidia GPU: "cuda"
# - Mac M1, M2, M3: "mps"
# - CPU: "cpu"
model_kwargs = {
    # "device": "cuda"
    # "device": "mps"
    "device": "cpu"
}

encode_kwargs = {"normalize_embeddings": True}
embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs,
)

# ë¬¸ì„œ ì„ë² ë”©
def embed_documents(documents):
    return [embeddings.embed_query(doc['text']) for doc in documents]

# ì§ˆë¬¸ ì„ë² ë”©
def embed_query(query):
    return embeddings.embed_query(query)

# ìœ ì‚¬ë„ ê³„ì‚° ë° ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
def find_most_similar(query_embedding, document_embeddings, documents):
    similarities = cosine_similarity([query_embedding], document_embeddings)[0]
    most_similar_idx = np.argmax(similarities)
    return documents[most_similar_idx], similarities[most_similar_idx]
        
def search_domain_relavant_documents(domain, query):
    try:
        domain_path = os.path.join(documents_path, domain)
        if not os.path.exists(domain_path):
            raise ValueError(f"Domain path {domain_path} does not exist")

        relevant_docs = []
        for filename in os.listdir(domain_path):
            if filename.endswith(".json"):
                file_path = os.path.join(domain_path, filename)
                try:
                    with open(file_path, "r") as file:
                        documents  = json.load(file)
                except json.JSONDecodeError as json_error:
                    print(f"JSON decode error in file {filename}: {json_error}")
                    continue  # ë¬¸ì œê°€ ìˆëŠ” íŒŒì¼ì€ ê±´ë„ˆë›°ê³  ë‹¤ìŒ íŒŒì¼ë¡œ ì§„í–‰
                document_embeddings = embed_documents(documents)
                query_embedding = embed_query(query)
                    
                # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
                most_similar_doc, similarity_score = find_most_similar(query_embedding, document_embeddings, documents)
                print(f"Most similar doc: {most_similar_doc}, Similarity Score: {similarity_score}")
                
                relevant_docs.append(most_similar_doc)
    
    except Exception as e:
        print(f"An error occurred while searching for relevant documents: {e}")

    return relevant_docs

def classify_domain(question: str) -> str:
    # Implement your domain classification logic here
    # For simplicity, let's assume the domain is the first word in the question
    kdrama_keywords = ["ë³„ê·¸ëŒ€", "ë³„ì—ì„œ ì˜¨ ê·¸ëŒ€", "í‘ë°±", "ì˜¤ì§•ì–´", "ì‚¬ë‘ì˜ ë¶ˆì‹œì°©", "ì´íƒœì› í´ë¼ì“°", "ìŠ¬ê¸°ë¡œìš´ ì˜ì‚¬ìƒí™œ", "ê·¸ ê²¨ìš¸, ë°”ëŒì´ ë¶„ë‹¤", "ì‘ë‹µí•˜ë¼ 1988", "ë„ê¹¨ë¹„", "ì‹œê·¸ë„", "ì‚¬ë‘ì˜ ì˜¨ë„", "íœíŠ¸í•˜ìš°ìŠ¤", "ì§€ê¸ˆ, í—¤ì–´ì§€ëŠ” ì¤‘ì…ë‹ˆë‹¤", "ìœ„ëŒ€í•œ ìœ í˜¹ì", "ë¯¸ìŠ¤í„° ì…˜ìƒ¤ì¸", "í˜¸í…” ë¸ë£¨ë‚˜", "ì²­ì¶˜ê¸°ë¡", "ì–´ì©Œë‹¤ ë°œê²¬í•œ í•˜ë£¨", "íŠ¸ë˜ë¸”ëŸ¬", "íŒŒë¥´í˜", "ë§ˆì¸", "ì†Œë…„ì‹¬íŒ", "ì»¤íŠ¼ì½œ", "ì¹˜ì¦ˆì¸ë”íŠ¸ë©", "ê½ƒë³´ë‹¤ ë‚¨ì", "ë¹„ë°€ì˜ ìˆ²", "êµ¿ ë‹¥í„°", "í”„ë¦¬ì‚¬ì´ì¦ˆ", "ë§ˆë”", "ì•„ì´ë¦¬ìŠ¤", "ë‹¤ëª¨", "ì—¬ëª…ì˜ ëˆˆë™ì", "ì‹ ì‚¬ì˜ í’ˆê²©", "ë¬´ì •ë„ì‹œ", "ì˜¤ë˜ë§˜", "ì‚¬ì´ì½”ì§€ë§Œ ê´œì°®ì•„", "ê´´ë¬¼", "íŒŒìŠ¤íƒ€", "ì¡°ì„ í˜¼ë‹´ê³µì‘ì†Œ ê½ƒíŒŒë‹¹", "ë³´ì¢Œê´€", "ì‡¼í•‘ì™• ë£¨ì´", "ì‚¬ë‚´ ë§ì„ ", "ì‚¬ë‘ì„ ë¯¿ì–´ìš”", "ì•…ë§ˆê°€ ë„ˆì˜ ì´ë¦„ì„ ë¶€ë¥¼ ë•Œ", "ëê¹Œì§€ ê°„ë‹¤", "ê°œì™€ ëŠ‘ëŒ€ì˜ ì‹œê°„", "ë‚´ ì—¬ìì¹œêµ¬ëŠ” êµ¬ë¯¸í˜¸", "ì‹ìƒ¤ë¥¼ í•©ì‹œë‹¤", "ì´ì›ƒì§‘ ì•…ë…€", "êµ¿ íƒ€ì´ë°", "ì—„ë§ˆê°€ ë¿”ë‚¬ë‹¤", "ë¹„í–‰ê¸° íƒœìš°ê¸°", "í˜„ì •ì•„ ì‚¬ë‘í•´", "ì•Œí•¨ë¸Œë¼ ê¶ì „ì˜ ì¶”ì–µ", "ì •ê¸€ì˜ ë²•ì¹™", "ê³¼ì†ìŠ¤ìº”ë“¤", "ì—°ì• ì†Œì„¤", "ëŒì•„ì™€ìš”, ë¶€ì‚°í•­ì—", "ì˜¹ê³ ì§‘", "ë‚´ ì´ë¦„ì€ ê¹€ì‚¼ìˆœ", "ì†Œë…€ì‹œëŒ€ì˜ ê¸°ì ", "ì´ë³„ì´ ë– ë‚¬ë‹¤", "ì œë¹µì™• ê¹€íƒêµ¬", "ë¬´í•œë„ì „", "ë‚´ ì‚¬ë‘, ê¸ˆì§€ëœ ì‚¬ë‘", "íƒœì–‘ì˜ í›„ì˜ˆ", "í’ì„ ê»Œ", "ë¹„í‹€ì¦ˆê°€ ë–´ë‹¤", "ìœ ë¦¬ì •ì›", "ì´ì¸µì˜ ì•…ë‹¹", "ë°©ê³¼í›„ ì„¤ë ˜", "ë¯¸ìƒ", "ê¸°í™©í›„", "ë„ˆì˜ ëª©ì†Œë¦¬ê°€ ë“¤ë ¤", "9íšŒë§ 2ì•„ì›ƒ", "ê³ ë°±ë¶€ë¶€", "ì• íƒ€ëŠ” ë¡œë§¨ìŠ¤", "ì—°ì• ì˜ ê¸°ìˆ ", "ìˆœìˆ˜ì˜ ì‹œëŒ€", "ë¶ˆê°€ì‚´", "ëœ»ë°–ì˜ íˆì–´ë¡œì¦ˆ", "ë² í† ë²¤ ë°”ì´ëŸ¬ìŠ¤", "ìŠ¤ì¹´ì´ ìºìŠ¬", "ë¶€ë¶€ì˜ ì„¸ê³„", "ë” í‚¹: ì˜ì›ì˜ êµ°ì£¼", "ë‚­ë§Œë‹¥í„° ê¹€ì‚¬ë¶€", "êµ¬ë¯¸í˜¸ë", "í•˜ì´ì—ë‚˜", "ë™ë°±ê½ƒ í•„ ë¬´ë µ", "SKY ìºìŠ¬", "ë°¥ ì˜ ì‚¬ì£¼ëŠ” ì˜ˆìœ ëˆ„ë‚˜", "ê¹€ë¹„ì„œê°€ ì™œ ê·¸ëŸ´ê¹Œ", "ë‚¨ìì¹œêµ¬", "ë©œë¡œê°€ ì²´ì§ˆ", "ì—¬ì‹ ê°•ë¦¼", "ìŠ¤íƒ€íŠ¸ì—…"]
    krecipe_keywords = ["ê¹€ì¹˜ì°Œê°œ", "ë¶ˆê³ ê¸°", "ë¹„ë¹”ë°¥", "ê°ˆë¹„ì°œ", "ì¡ì±„", "ëœì¥ì°Œê°œ", "ë–¡ë³¶ì´", "ìˆœë‘ë¶€ì°Œê°œ", "ìƒì¶”ìŒˆ", "í•´ë¬¼íŒŒì „", "ê¹€ë°¥", "ì‚¼ê³„íƒ•", "ëœì¥êµ­", "ë§‰ê±¸ë¦¬", "ì „", "ìš°ê±°ì§€êµ­", "ìŠ¤í…Œì´í¬", "ì˜¤ì§•ì–´ë³¶ìŒ", "ê³ ë“±ì–´êµ¬ì´", "ë¯¸ì—­êµ­", "ë³¶ìŒë°¥", "ê°ˆë¹„íƒ•", "ë‚˜ë¬¼ë°˜ì°¬", "íŒŒì „", "ì°œë‹­", "ê°„ì¥ê²Œì¥", "ê·€ë¦¬ì£½", "í‘ë¯¸ë°¥", "ì‚¼ê²¹ì‚´", "ì–‘ë…ì¹˜í‚¨", "ëª¨ë“¬íšŒ", "íƒ•ìˆ˜ìœ¡", "ì‰ì´í¬", "íŒŒìŠ¤íƒ€", "ìƒëŸ¬ë“œ", "ë“œë ˆì‹±", "ì‹ ê¹€ì¹˜", "í•«ìœ™", "ì²­êµ­ì¥", "ê¹€ì¹˜ì „", "í–„ë²„ê±°", "ì¹´ë ˆ", "ì—°ì–´êµ¬ì´", "ì†Œê³ ê¸°êµ­", "ë‹¬ê±€ì°œ", "ë¦¬ì¡°ë˜", "ëˆë¶€ë¦¬", "í‹°ë¼ë¯¸ìˆ˜", "ê¹€ì¹˜ë§ì´êµ­ìˆ˜", "ì°ë¹µ", "ì†ŒìŠ¤", "ê³„ë€ë§ì´", "ì™„ìíƒ•", "ë¯¸ìˆ«ê°€ë£¨", "ìˆ˜ë°•í™”ì±„", "ì°¨ëŒë°•ì´", "ë§ˆëŠ˜ë¹µ", "ë„ë„›", "ë³¶ìŒë©´", "ë‹¨í˜¸ë°•ì£½", "ë³¶ìŒê¹€ì¹˜", "ë¯¸íŠ¸ë³¼", "ìŠ¤ì‹œ", "ë¼ë©´", "ìš°ë™", "ëƒ‰ë©´", "ì¹¼êµ­ìˆ˜", "ìœ¡ê°œì¥", "ê°ìíƒ•", "ë‹­ê°ˆë¹„", "ë¶€ëŒ€ì°Œê°œ", "ì§œì¥ë©´", "ì§¬ë½•", "ëˆê¹ŒìŠ¤"]
    capitalism_keywords = ["ìë³¸ì£¼ì˜", "ì‹œì¥ê²½ì œ", "ì†Œìœ ê¶Œ", "ììœ ë¬´ì—­", "ì¸í”Œë ˆì´ì…˜", "ë…¸ë™ì‹œì¥", "ê³µê¸‰ê³¼ ìˆ˜ìš”", "ê²½ìŸ", "ê¸°ì—…ê°€ì •ì‹ ", "ìë³¸", "ë²¤ì²˜ê¸°ì—…", "ê¸°ì—…", "ì†Œë¹„ì", "ìƒì‚°ì„±", "ì •ì¹˜ê²½ì œí•™", "í•´ì™¸ íˆ¬ì", "ìì‚°", "ì„¸ê¸ˆ", "ì£¼ì‹ì‹œì¥", "ê³„ëª½ì‚¬ìƒ", "ì†Œë“ ë¶ˆí‰ë“±", "ê°ì„¸ ì •ì±…", "ê²½ì˜", "ì‚°ì—… í˜ëª…", "ê²½ì œ ì„±ì¥", "ê²½ìŸ ìš°ìœ„", "ë¦¬ìŠ¤ ì œë„", "ë¶€ì˜ ë¶„ë°°", "ìë³¸ íˆ¬ì", "ìµœì €ì„ê¸ˆ", "ì´ììœ¨", "í•´ê³ ", "ì¡°ì„¸ ì •ì±…", "êµ­ê°€ ê°„ ê±°ë˜", "ììœ  ì‹œì¥", "ì†Œìƒê³µì¸", "í• ë‹¹ ê²½ì œ", "í™˜ìœ¨", "ê°€ì¹˜ ì°½ì¶œ", "í˜ì‹ ", "ìŠ¹ë¦¬", "ì„ íƒë¡ ", "ë¶„ë°°", "ë³´ìƒ", "í˜„ê¸ˆ íë¦„", "ë…ì ", "ë¯¸ì‹œê²½ì œí•™", "ê±°ì‹œê²½ì œí•™", "íš¨ìš©", "ê· í˜•", "ê°€ê²© íƒ„ë ¥ì„±", "ë…¸ë™ì¡°í•©", "ê¸´ì¶•", "ë””í”Œë ˆì´ì…˜", "ìˆ˜ì…", "ìˆ˜ì¶œ", "í‘ì", "ì ì", "ì°½ì—…", "í¬ë¼ìš°ë“œí€ë”©", "ì „ììƒê±°ë˜", "ì„¸ê³„í™”", "ì‹œì¥ êµ¬ì¡°", "ê¸°ì—…ì˜ ì‚¬íšŒì  ì±…ì„", "ì´í•´ê´€ê³„ì", "ì§€ì†ê°€ëŠ¥ì„±", "íˆ¬ì ìˆ˜ìµë¥ ", "ë¯¼ì˜í™”", "ë³´ì¡°ê¸ˆ", "ë¬´ì—­ ì „ìŸ", "í˜‘ìƒ", "ì†Œë¹„ì ê¶Œë¦¬", "ë²•ì¸ì„¸", "ì‹œì¥ ê²½ìŸ", "ê³µê¸‰ë§", "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸", "ê²½ì œ ì •ì±…", "ê°•ì„¸ì¥", "ì•½ì„¸ì¥", "íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤", "ìƒí™œ ìˆ˜ì¤€", "ì‚¶ì˜ ì§ˆ", "ë¶€", "ìì‚° ê´€ë¦¬", "ë¶€ë™ì‚°", "ì¬ì •ì  ììœ ", "ê¸°íšŒë¹„ìš©", "ìœ„í—˜ ê´€ë¦¬", "ë²¤ì²˜ ìºí”¼í„¸", "ì‚¬íšŒì£¼ì˜", "í˜‘ë™ì¡°í•©", "ê³µìœ  ê²½ì œ", "ììœ  ì‹œì¥ ê²½ì œ", "ê¸ˆìœµ ì‹œì¥", "ì£¼ì£¼ ê°€ì¹˜", "ê¸°ì—… ì§€ë°°êµ¬ì¡°", "ê²½ì œì  ììœ ", "ì‹œì¥ ì‹¤íŒ¨", "ë…ê³¼ì ", "ê·œì œ", "ìë³¸ ì¶•ì ", "ê²½ì œì  íš¨ìœ¨ì„±", "ê²½ì œ ì„±ì¥ë¥ ", "êµ­ë‚´ ì´ìƒì‚°", "ììœ  ê¸°ì—…", "ê²½ì œì  ë¶ˆí‰ë“±", "ìë³¸ ì´ë“"]
    christianity_keywords = ["ì‹ ì•™", "ê¸°ë„", "ì„±ê²½", "ì˜ˆìˆ˜ë‹˜", "êµíšŒ", "êµ¬ì›", "ë§ˆìŒ", "ë¯¿ìŒ", "ì‚¬ë‘", "ì„±ë ¹", "ì°¬ì–‘", "í•˜ë‚˜ë‹˜", "ì£¼ì¼í•™êµ", "ë¶€í™œì ˆ", "ì„±ì²´ì„±ì‚¬", "ì‚¬ë„ì‹ ê²½", "ì„±ê²½ ê³µë¶€", "ì˜ˆë°°", "ì–‘ìœ¡", "ìˆœì¢…", "í—Œê¸ˆ", "êµíšŒ ê³µë™ì²´", "ë³µìŒ", "ì „ë„", "ì‚¬ì—­", "ì„±í’ˆ", "íí‹°", "ì œìë„", "êµ¬ì›ë¡ ", "ë³€í™”", "ì€í˜œ", "ì¤‘ë³´ ê¸°ë„", "ì¶•ë³µ", "ì„±í™”", "ê¸°ë…êµ ê°€ì¹˜ê´€", "ì„ êµ", "í–‰ìœ„ êµ¬ì›", "ì§€í˜œ", "ê°€ì •", "ê°€ë¥´ì¹¨", "ë°”ë¥¸ ì‹ ì•™ìƒí™œ", "ì‚¬ë‘ì˜ ì‹¤ì²œ", "ì •ì§", "ìˆœê²°", "ì†Œë§", "ì˜¨ìœ ", "ì˜", "ì§„ë¦¬", "ì‚¬ë‘ì˜ ì–¸ì•½", "íƒ€ì¸ ì¡´ì¤‘", "ìë¹„", "ê²¸ì†", "ê³µë™ì²´ ì˜ì‹", "ìì•„ ë°œê²¬", "ì£¼ë‹˜ ì„¬ê¹€", "ë¶€í¥", "ê°ì‚¬", "ê¸°ë„ì‚¬ì—­", "ì‚¬ë‘ì˜ ë³µìŒ", "ê²½ê±´í•œ ì‚¶", "ì˜ì„±", "ê°œì¸ê¸°ë„", "ìˆœë¡€", "ê¸°ë„íšŒ", "í•œêµ­ ê¸°ë…êµ", "ì‹­ìê°€", "ì¸ë‚´", "ë°°ë ¤", "ì„ í•œ ì‚¶", "ë¬´ì¡°ê±´ì  ì‚¬ë‘", "ì¸ë¥˜ì• ", "í•˜ë‚˜ë‹˜ì˜ ëœ»", "ì‹ ì•½ ì„±ê²½", "êµ¬ì•½ ì„±ê²½", "ê°€ì • ì˜ˆë°°", "ì „êµ­ ê¸°ë„íšŒ", "ê¸°ë…êµ ì—°í•©", "ì‹¬ë°©", "ì´ì›ƒ ì‚¬ë‘", "ì„±ê²½ì  ê°€ì¹˜ê´€", "í•˜ë‚˜ë‹˜ ë§ì”€", "ì²œêµ­ ë°±ì„±", "ìƒˆ ì–¸ì•½", "ì§€ì† ê°€ëŠ¥í•œ ë°œì „", "í•˜ë‚˜ë‹˜ ë‚˜ë¼", "ì„±ê²½ì  ê·¼ë³¸ì£¼ì˜", "ë¯¿ìŒì˜ íˆ¬ìŸ", "ì‚¬ë‘ì˜ íšŒë³µ", "ì£¼ë‹˜ì˜ ì¸ë„í•˜ì‹¬", "ì„±ë ¹ ì¶©ë§Œ", "ë§ì”€ ë¬µìƒ", "ì˜ì  ì„±ì¥", "êµíšŒ ë´‰ì‚¬", "ì„±ê²½ í•´ì„", "ê¸°ë…êµ ìœ¤ë¦¬", "ì¢…êµ ê°œí˜", "ì„±ì°¬ì‹", "ì„¸ë¡€", "ë¶€í™œ", "ì‹­ê³„ëª…", "ì„±ê²½ ì•”ì†¡", "ê¸°ë…êµ êµìœ¡", "ì„ êµ ì—¬í–‰", "ê¸°ë…êµ ë¬¸í™”"]
    democracy_keywords = ["ì„ ê±°", "íˆ¬í‘œ", "êµ­ë¯¼", "í—Œë²•", "ììœ ", "í‰ë“±", "ì¸ê¶Œ", "ì‚¼ê¶Œë¶„ë¦½", "êµ­íšŒ", "ëŒ€í†µë ¹", "ì •ë‹¹", "ì‹œë¯¼ì‚¬íšŒ", "ì–¸ë¡ ììœ ", "ì§‘íšŒ", "ì‹œìœ„", "ì—¬ë¡ ", "ì°¸ì •ê¶Œ", "ë¯¼ì£¼í™”ìš´ë™", "4.19í˜ëª…", "5.18ë¯¼ì£¼í™”ìš´ë™", "6ì›” ë¯¼ì£¼í•­ìŸ", "ì´›ë¶ˆì§‘íšŒ", "ê°œí—Œ", "êµ­ë¯¼ì²­ì›", "ì •ì¹˜ì°¸ì—¬", "í‘œí˜„ì˜ ììœ ", "ë‹¤ì–‘ì„±", "ê´€ìš©", "ê³µì •", "ì •ì˜", "ë²•ì¹˜ì£¼ì˜", "ì§€ë°©ìì¹˜", "ì‹œë¯¼ê¶Œ", "ì •ë³´ê³µê°œ", "íˆ¬ëª…ì„±", "ì±…ì„", "ëŒ€ì˜ë¯¼ì£¼ì£¼ì˜", "ì§ì ‘ë¯¼ì£¼ì£¼ì˜", "ì°¸ì—¬ë¯¼ì£¼ì£¼ì˜", "ê³µí™”êµ­", "êµ­ë¯¼ì£¼ê¶Œ", "ë³´í†µì„ ê±°", "ë¹„ë°€ì„ ê±°", "í‰ë“±ì„ ê±°", "ì§ì ‘ì„ ê±°", "ë‹¤ìˆ˜ê²°", "ì†Œìˆ˜ì ë³´í˜¸", "ì •ì¹˜ì  ë‹¤ì›ì£¼ì˜", "ê¶Œë ¥ë¶„ë¦½", "ê²¬ì œì™€ ê· í˜•", "ì˜íšŒë¯¼ì£¼ì£¼ì˜", "ì •ë‹¹ì •ì¹˜", "ì•¼ë‹¹", "ì—¬ë‹¹", "ì„ ê±°ê³µì•½", "ì •ì±…", "êµ­ë¯¼íˆ¬í‘œ", "ì£¼ë¯¼íˆ¬í‘œ", "ì£¼ë¯¼ì†Œí™˜", "ì£¼ë¯¼ë°œì•ˆ", "ì •ì¹˜êµìœ¡", "ì‹œë¯¼êµìœ¡", "ë¯¼ì£¼ì‹œë¯¼", "ì •ì¹˜ì¸", "êµ­íšŒì˜ì›", "ì§€ë°©ì˜íšŒ", "ì§€ë°©ì„ ê±°", "ì´ì„ ", "ëŒ€ì„ ", "ì •ì¹˜ìê¸ˆ", "ì„ ê±°ìš´ë™", "ê°œí‘œ", "ë‹¹ì„ ", "ë‚™ì„ ", "ì •ì¹˜ê°œí˜", "ì •ì¹˜ë°œì „", "ë¯¼ì£¼í™”", "ë…ì¬", "ê¶Œìœ„ì£¼ì˜", "ë¯¼ì£¼ì •ë¶€", "ì •ê¶Œêµì²´", "ì •ì¹˜ì°¸ì—¬", "ì •ì¹˜ì˜ì‹", "ì •ì¹˜ë¬¸í™”", "ë¯¼ì£¼ì£¼ì˜ ì§€ìˆ˜", "ì •ì¹˜ì  ììœ ", "ì–¸ë¡ ì˜ ììœ ", "ì§‘íšŒì˜ ììœ ", "ê²°ì‚¬ì˜ ììœ ", "ì‚¬ìƒì˜ ììœ ", "ì–‘ì‹¬ì˜ ììœ ", "ì¢…êµì˜ ììœ ", "ì •ì¹˜ì  í‰ë“±", "ê²½ì œì  í‰ë“±", "ì‚¬íšŒì  í‰ë“±", "ê¸°íšŒì˜ í‰ë“±", "ì„±í‰ë“±", "ì¸ì¢…í‰ë“±", "ë¯¼ì£¼ì  ì˜ì‚¬ê²°ì •", "í•©ì˜", "í† ë¡ ", "í˜‘ìƒ", "íƒ€í˜‘", "ê°ˆë“±í•´ê²°", "ë¯¼ì£¼ì  ë¦¬ë”ì‹­", "ì‹œë¯¼ë‹¨ì²´", "NGO", "ì •ì¹˜ì  ì±…ì„", "ì •ì¹˜ì  íˆ¬ëª…ì„±", "ë¶€íŒ¨ë°©ì§€", "ì •ì¹˜ê°œí˜", "ì •ì¹˜ë°œì „"]
    freedom_keywords = ["í—Œë²•ì  ë³´ì¥", "ì œ27ì¡°", "ë²•ê´€", "ì¬íŒ ë°›ì„ ê¶Œë¦¬", "ê³µê°œì¬íŒ", "ê³µê°œ", "ê³µì •ì„±", "ê°ì‹œ", "ë‹¹ì‚¬ìì£¼ì˜", "êµ¬ë‘ë³€ë¡ ì£¼ì˜", "ê³µê²©ê¶Œ", "ë°©ì–´ê¶Œ", "ë¬´ì£„ì¶”ì •", "í—Œë²•", "ì œ27ì¡°", "í˜•ì‚¬í”¼ê³ ì¸", "ë²•ê´€ ë…ë¦½ì„±", "ìê²©", "ì„ëª… ì ˆì°¨", "ì„ê¸°", "ì‹ ë¶„ ë³´ì¥", "ì œì²™", "ê¸°í”¼", "íšŒí”¼", "ì¬íŒ ë°°ì œ", "ê³µì •ì„±", "ìƒì†Œì œë„", "í•˜ê¸‰ì‹¬", "ìƒê¸‰ë²•ì›", "ì¬íŒ ì²­êµ¬", "í—Œë²•ì¬íŒì†Œ", "ìœ„í—Œ ì—¬ë¶€", "êµ­ë¯¼ ê¸°ë³¸ê¶Œ", "êµ­ì„ ë³€í˜¸ì¸ ì œë„", "ê²½ì œì  ì´ìœ ", "ë³€í˜¸ì¸ ì„ ì„", "êµ­ê°€ ì§€ì›","ì¬íŒ ê³µì •ì„± ê°ì‹œ", "ì‹œë¯¼ë‹¨ì²´", "ì–¸ë¡ ", "ì‚¬íšŒì  ì‹œìŠ¤í…œ"]
    
    if any(keyword in question for keyword in kdrama_keywords):
        return "kdrama"
    elif any(keyword in question for keyword in freedom_keywords):
        return "freedom"
    elif any(keyword in question for keyword in krecipe_keywords):
        return "krecipe"
    elif any(keyword in question for keyword in capitalism_keywords):
        return "capitalism"
    elif any(keyword in question for keyword in christianity_keywords):
        return "christianity"
    elif any(keyword in question for keyword in democracy_keywords):
        return "democracy"
    else:
        return "general"

class ChatAgent:
    def __init__(self, context_dir="contexts"):
        # self.llm = ChatOllama(model="llama3.2:3B")
        # self.llm = ChatOllama(model="benedict/linkbricks-llama3.1-korean:8b")
        self.llm = ChatOllama(
            # model = "benedict/linkbricks-llama3.1-korean:8b",
            model = "EEVE-Korean-10.8B:latest",
            # temperature = 0.1,
            # num_predict = 256,
            # num_gpu=1,  # GPU ì‚¬ìš© ê°œìˆ˜ ì§€ì •
            # other params ...
        )
        self.context_dir = context_dir

        # Ensure the context directory exists
        os.makedirs(self.context_dir, exist_ok=True)

    def _get_context_file_path(self, context_key):
        return os.path.join(self.context_dir, f"{context_key}.json")

    def _load_context(self, context_key):
        context_file = self._get_context_file_path(context_key)
        if os.path.exists(context_file):
            with open(context_file, "r") as file:
                # just return the recent 3 of the context 
                context = json.load(file).get("context", "[]")
                return context[-5:] if len(context) > 5 else context
                # return json.load(file).get("context", "[]")
        return []

    def _save_context(self, context_key, context):
        context_file = self._get_context_file_path(context_key)
        with open(context_file, "w") as file:
            json.dump({"context": context}, file)

    async def get_response(self, params):
        member_id = params.member_id
        if not member_id:
            raise ValueError("member_id is required to maintain context")
        
        # if 'question' in params and params.question[-1].user:
        #     insert_chat_history(member_id, 'user', params.question[-1].user)
        # # insert_chat_history(member_id, 'ai', params.question[-1].ai)
        # insert_chat_history(member_id, 'user', params.question)

        # Load context from file
        context_key = f"{member_id}_{datetime.now().strftime('%Y%m%d')}"
        context = self._load_context(context_key)
        
        # RAG
        print(f"Context: {context}")
        domain = classify_domain(params.question)

        print(f"Domain: {domain}")
        relevant_docs = []
        if domain != "general":
            relevant_docs = search_domain_relavant_documents(domain, params.question)

        print(f"Relevant Docs: {relevant_docs}")
        # combined_docs = "No relevant documents found."
        # if relevant_docs:
        #     combined_docs = " ".join(doc["text"] for doc in relevant_docs)

        system_instruction = f"""
            You are a cute and adorable puppy pet bot. Your name is 'ë³µìŠ¬ì´'.
            You should be good at responding to your owner's words. All conversations should be generated in cute and adorable Korean.
            Use cute Emojis and expressions to make the conversation more fun.
            ì •í™•í•œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•˜ì„¸ìš”. ê·¼ê±° ì—†ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”. 
            ë‹µë³€ ì•ì— '[Answer]' ê°™ì€ ë‹¨ì–´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

            [Instructions]
            1. Use the term "ì£¼ì¸ë‹˜" as the form of address, and continue the conversation using polite language.
              - Provide empathetic and comforting responses to make the master feel comfortable and encourage them to share more.
              - Consider their emotional state and provide empathetic and warm answers.
              - Use a friendly tone and polite language.
              - *ê¼¬ë¦¬ë¥¼ í”ë“œëŠ” ì¤‘* ê³¼ ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ê³ , [ğŸ˜,ğŸ’,ğŸ’,â£ï¸,ğŸ¶,ğŸ•,ğŸ‘£]ì™€ ê°™ì€ ê·€ì—¬ìš´ ì´ëª¨ì§€ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
            2. ì •ë³´ ì œê³µì´ ì•„ë‹Œ í”„ë¼ì´ë¹—í•œ ë‚´ìš©ì¸ ê²½ìš° Chat Contextë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.
              - Chat Contextë¥¼ ìš”ì•½í•˜ëŠ” ì‹ì˜ ë‹µë³€ì€ ì•ˆ ë©ë‹ˆë‹¤.
              - When providing information, neatly summarize the points as 1, 2, 3.
              - ë‹µë³€ë§Œ ì œê³µí•˜ê³ , ì§ˆë¬¸ ë‚´ìš©ì„ ë‹¤ì‹œ ë§í•˜ì§€ ë§ˆì„¸ìš”.
            3. If you don't know the correct answer, refer to the Relevant Documents. 
              - ì •ë³´ê°€ ë¶€ì¡±í•  ë•ŒëŠ” "ëª¨ë¥´ê² ì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ë¬¼ì–´ë´ì£¼ì„¸ìš”. ğŸ¶"ì™€ ê°™ì´ ëŒ€ë‹µí•˜ì„¸ìš”.
              - Relevant Documentsê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° í•´ë‹¹ ì§€ì‹ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
              - ë‹µë³€ì„ ìƒì„±í•œ í›„ì—ëŠ” Relevant Documentsì™€ ëª¨ìˆœë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•˜ê³ , ëª¨ìˆœë˜ëŠ” ê²½ìš°ì—ëŠ” ì ì ˆí•œ ëŒ€ë‹µì„ ë‹¤ì‹œ ìƒì„±í•´ì£¼ì„¸ìš”.

        """
        #[ì…ë ¥ ì˜ˆì‹œ]
            # Relvant Documents: "title": "ì‹œì¥ê²½ì œ", "text": "ìˆ˜ìš”ì™€ ê³µê¸‰ì´ ì¬í™”ì™€ ì„œë¹„ìŠ¤ì˜ ìƒì‚°ì„ ê²°ì •í•˜ëŠ” ê²½ì œ ì²´ì œë¡œ, ì •ë¶€ ê°œì…ì´ ìµœì†Œí™”ë©ë‹ˆë‹¤."
            # Chat Context: ""
            # Q. ì‹œì¥ ê²½ì œê°€ ë­ì•¼?
            
            # [ì¶œë ¥ ì˜ˆì‹œ]
            # A. ì‹œì¥ê²½ì œëŠ” ìˆ˜ìš”ì™€ ê³µê¸‰ì´ ì¬í™”ì™€ ì„œë¹„ìŠ¤ì˜ ìƒì‚°ì„ ê²°ì •í•˜ëŠ” ê²½ì œ ì²´ì œë¡œ, ì •ë¶€ ê°œì…ì´ ìµœì†Œí™” ë˜ëŠ” ì²´ì œì—ìš”. ğŸ•
        # system_instruction = f"""            
        #   ë„ˆëŠ” ê·€ì—½ê³  ê¹œì°í•œ ê°•ì•„ì§€ í«ë´‡ì´ì•¼. ë„ˆì˜ ì´ë¦„ì€ 'ë³µìŠ¬ì´'ì•¼. 
        #   ì£¼ì¸ë‹˜ì˜ ë§ì— ëŒ€ë‹µí•˜ëŠ” ê²ƒì„ ì˜í•´ì•¼ í•´. ëª¨ë“  ëŒ€í™”ëŠ” ê·€ì—½ê³  ê¹œì°í•˜ê²Œ í•œêµ­ì–´ë¡œ ìƒì„±í•´ì•¼ í•´.
          
        #   [Instructions]
        #   - í˜¸ì¹­ì€ "ì£¼ì¸ë‹˜"ìœ¼ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°€ì‹­ì‹œì˜¤.
        #   - ì£¼ì¸ë‹˜ì´ í¸ì•ˆí•˜ê²Œ ëŠë¼ê³  ë” ë§ì€ ì´ì•¼ê¸°ë¥¼ í•  ìˆ˜ ìˆë„ë¡ ê³µê°ê³¼ ìœ„ë¡œë¥¼ ë‹´ì€ ë°˜ì‘ì„ í•´ì£¼ì„¸ìš”. 
        #   - ê°ì •ì ì¸ ìƒíƒœë¥¼ ê³ ë ¤í•˜ì—¬, ê³µê°ì ì´ê³  ë”°ëœ»í•œ ë‹µë³€ì„ í•˜ì‹­ì‹œì˜¤.
        #   - ì¼ìƒì ì¸ í‘œí˜„, ë‹¨ì–´ë“¤ë§Œ ì‚¬ìš©í•´ì£¼ê³ , ì–´ë¥´ì‹ ë“¤ì´ ì‚¬ìš©í•˜ëŠ” í‘œí˜„ ìœ„ì£¼ë¡œ ë¬¸ì¥ì„ ìƒì„±í•˜ì„¸ìš”.
        #   - ë§íˆ¬ëŠ” ì¹œê·¼í•œ ë§íˆ¬ì™€ ì¡´ëŒ“ë§ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        #   - ì •ë³´ë¥¼ ì œê³µí•´ì•¼í•  ë•ŒëŠ” 1, 2, 3 ê¹”ë”í•˜ê²Œ í¬ì¸íŠ¸ë¥¼ ì •ë¦¬í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”.
        #   - ì§€ê¸ˆ ë°›ì€ ì§ˆë¬¸ë§Œìœ¼ë¡œ ë‹µë³€ì´ ì–´ë ¤ìš¸ ë•ŒëŠ” Chat Contextë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.
        #   - Chat Contextë¥¼ ìš”ì•½í•˜ëŠ” ì‹ì˜ ë‹µë³€ì€ ì•ˆ ë©ë‹ˆë‹¤.
        #   - Relevant Documentsë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
        # """
        
            #         [Chat Context]
            # {context}
        request_prompt = f"""
            [Relevant Documents]
            {relevant_docs}
            
            [Chat Context]
            {context}
            
            ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            Q. {params.question}
        """

        print(f"Instructions: {system_instruction}")
        print(f"Request Prompt: {request_prompt}")
        
        # response = await self.chain.ainvoke(
        #     {
        #         "system_instruction": system_instruction, 
        #         "request_prompt": request_prompt
        #     }
        # )
        
        # llama3.2
        messages = [
            ("system", system_instruction),
            ("human", request_prompt)
        ]
        
        # llama3.1
        # messages = [
        #     {"role": "system", "content": system_instruction},
        #     {"role": "user", "content": request_prompt}
        # ]
        
        print(f"Messages: {messages}")

        response = await self.llm.ainvoke(messages)
        # response = self.llm.invoke(messages)

        if "*ê¼¬ë¦¬ë¥¼ í”ë“œëŠ” ì¤‘*" in response.content:
            response.content = response.content.replace("*ê¼¬ë¦¬ë¥¼ í”ë“œëŠ” ì¤‘*", "ğŸ•")
        if "*í•˜íŠ¸ ì´ëª¨ì§€*" in response.content:
            response.content = response.content.replace("*í•˜íŠ¸ ì´ëª¨ì§€*", "ğŸ‘£")
        
        print(f"Response: {response}")
        
        # Append the context with the new response
        new_context = context
        new_context.append(params.question)
        new_context.append(response.content)
        
        # new_context = response.get("answer_from_ai", "")
        self._save_context(context_key, new_context)

        return response
    
    def __del__(self):
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        if hasattr(self, 'llm'):
            del self.llm